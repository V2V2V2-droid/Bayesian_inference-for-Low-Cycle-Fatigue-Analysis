# -*- coding: utf-8 -*-
"""Bayesian_inference2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G-wZNnoZ_4pg1lN0Brw-Z-e2Z4IZt6yz
"""

import numpy as np
import pymc3 as pm
import arviz as az
az.rcParams["plot.matplotlib.show"] = True  
from scipy.stats import norm, kstest, ks_2samp
import scipy.stats as stats
import matplotlib.pyplot as plt

## input data:

# 1st option: plug it to the other script
# Bn initialization values are taken from the Experimental_data script: 
# from Experimental_data_safran import Bn_positive, Bn_positive_mean, Bn_positive_std
# bn_init = Bn_positive_mean
# bn_std_init = Bn_positive_std
# otherwise take as constant: 


# DIFFERENTIATE INITIAL BN FROM EXP DATA IN THE LIKELIHOOD AND THE BN IN THE PRIOR WHICH AR HE BELIEFS
bn_init_values =  (1.149, 0.7)
bn_priors = (1.45, 0.32)

# Constants (to put as parameters later on)

sigma_eps = 0.01
gamma = 6.63
N0_w = 79000 # Coming from the damage parameter calculation: taken as constant for now

observed_Nf = [10275, 19773, 50119, 171907, 5196, 25347, 50573, 100453, 11348, 12533, 15287, 135323, 17504, 19773,35562, 38922, 42793, 50119, 55855, 69058, 78364, 101364]

# fix one random state : 
RANDOM_SEED = 5690


Nf_list=[12000, 25000, 67000, 127000, 79000]
Nf_list = [50931, 43623, 45392, 63018]

# Dict of parameters used for likelihood function
likelihood_params = {
   'gamma': gamma,
   'sigma': bn_init_values[1],
   'sigma_eps': sigma_eps,
   'N0_w':N0_w,
   'Nf_list': Nf_list 
}



class Modified_Weibull(pm.Continuous):
    def __init__(self, bn, **likelihood_params):
      super().__init__()
      self.bn = bn
      self.mode = bn
      for key, value in likelihood_params.items():
            setattr(self, key, value)
      

    def logp(self, value=0):
      bn = self.bn
      return m_weibull_logp(bn -value, **likelihood_params)

def m_weibull_logp(bn, gamma, sigma, sigma_eps, N0_w, Nf_list):
    t=len(Nf_list)
    nf0 = N0_w*(np.log(2)**(1/bn))/gamma
    return (1/(((2*np.pi)**(t/2))*sigma_eps**t))*np.exp(-1/(2*sigma**2)*sum([(np.log(i) - np.log(nf0))**2 for i in Nf_list]))


class FatigueLife_BayesianCalibration:
  def __init__(self, sample_size, observations, bn_init_values, step_method=None, seed= RANDOM_SEED, **likelihood_params):
    self.sample_size = sample_size
    self.seed = seed
    self.observations = observations
    self.bn_init_mean = bn_init_values[0]
    self.bn_init_std = bn_init_values[1]
    self.step_method = step_method
    for key, value in likelihood_params.items():
            setattr(self, key, value)
    #self.inference_data = self.get_posterior_samples()
    
  def get_posterior_samples(self):
    with pm.Model() as m: 
      bn = pm.TruncatedNormal("bn", mu=bn_priors[0], sigma=bn_priors[1], lower=0.001)
      likelihood_bn = Modified_Weibull("slope", bn=bn, 
                                       **likelihood_params,
                                       observed = self.observations)
      if self.step_method == "Metropolis":
        step = pm.Metropolis() 
        idata = pm.sample(self.sample_size, cores=4, step=step, chains=2)
      else:
        idata = pm.sample(self.sample_size, cores=4, chains=2) # automatic determination of the step by the model
      s = az.summary(idata)
      print(s)
      az.plot_trace(idata)
      az.plot_posterior(idata)
      return idata, s
  

print(m_weibull_logp(bn_init_values[0], **likelihood_params))
A, s = FatigueLife_BayesianCalibration(sample_size = 5000, step="Metropolis", observations = observed_Nf, bn_init_values= bn_init_values).get_posterior_samples()

x=A["bn"]
bn_mean ) np.mean(x)

bn_mean = np.mean(x)

bn_mean

# define scoring for bn normality

# result: KS stat relatively small: good news but p value also very small: due to sample size but not only, 
# not great result for p value : meaning not obvious to reject or not the null (similarity) hypothesis. 
# if we look at smaller sample size: better on the p value  but not tremendous. 


normal = norm(loc = bn_priors[0], scale = bn_priors[1])
norm_rvs = normal.rvs(size=5000, random_state=RANDOM_SEED)
norm_rvs = norm_rvs[norm_rvs>0]


plt.hist(norm_rvs, color="red", label="CDF")
#plt.hist(x, color="green", label="X")
plt.show()

x = A["bn"]
#print(kstest(x,np.cumsum(norm_rvs), alternative = "two-sided"))

print(ks_2samp(x,norm_rvs, alternative = "two-sided", mode="asymp")[1])
print(ks_2samp(x[:50],norm_rvs[:50], alternative = "two-sided", mode="exact"))

print(ks_2samp(x,norm_rvs, alternative = "two-sided", mode="asymp"))

l = np.arange(start=1, stop = 5000, step=200)

#l = [1, 200, 500, 700, 1000, 1200, 1500, 1700, 2000, 2200, 2500, 2700, 3000, 3200, 3500, 3700, 4000, 4200, 4500, 4700, 5000]
k = []


for i in range(len(l)-1):
  k.append(ks_2samp(x[l[i]:l[i+1]],norm_rvs[l[i]:l[i+1]], alternative = "two-sided", mode="asymp")[1])

print(k)
print(np.average(k))

k_ = np.array(k)
print(len(k))
print(len(k_[k_<0.005]))